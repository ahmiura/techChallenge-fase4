# ğŸ“˜ EspecificaÃ§Ã£o do Projeto: Tech Challenge Fase 4
**Curso:** Machine Learning Engineering (Postech)
**Tema:** Deep Learning & Time Series Forecasting com LSTM

---

## 1. ğŸ¯ Objetivo do Projeto
Desenvolver uma arquitetura completa de **Deep Learning** para resolver um problema de **SÃ©rie Temporal**. O objetivo Ã© prever o preÃ§o de fechamento (*Close Price*) de uma aÃ§Ã£o da bolsa de valores utilizando redes neurais **LSTM (Long Short-Term Memory)**.

O projeto deve demonstrar maturidade em MLOps, cobrindo o ciclo de vida completo: ingestÃ£o de dados, experimentaÃ§Ã£o, treinamento, avaliaÃ§Ã£o e produtizaÃ§Ã£o via API containerizada.

---

## 2. ğŸ› ï¸ Stack TecnolÃ³gica & Ferramentas

### Essencial (ObrigatÃ³rio)
* **Linguagem:** Python 3.9+.
* **Framework de Deep Learning:** PyTorch.
* **Fonte de Dados:** Biblioteca `yfinance` (Yahoo Finance API).
* **ManipulaÃ§Ã£o de Dados:** Pandas, Numpy, Scikit-learn.
* **API Web:** FastAPI.
* **ContainerizaÃ§Ã£o:** Docker (CriaÃ§Ã£o de imagem e container).
* **Controle de VersÃ£o:** Git / GitHub.

### Recomendado (Para Arquitetura Limpa e Profissional)
* **Rastreamento de Experimentos:** MLflow (Para logar parÃ¢metros, mÃ©tricas e artefatos).
* **Estrutura de Projeto:** PadrÃ£o "Cookiecutter" ou estrutura modular separando `src` de `notebooks`.

---

## 3. ğŸ“œ Regras de NegÃ³cio e Pipeline de Dados

### 3.1. Coleta e IngestÃ£o
* **Fonte:** Utilizar dados histÃ³ricos diÃ¡rios do Yahoo Finance.
* **Ativo:** Escolher uma empresa com histÃ³rico consistente (ex: `PETR4.SA`, `VALE3.SA`, `ITUB3.SA`).
* **Janela Temporal:** Recomenda-se utilizar pelo menos 5 anos de dados para capturar sazonalidades.

### 3.2. PrÃ©-processamento (CrÃ­tico para LSTMs)
* **NormalizaÃ§Ã£o:** Ã‰ **obrigatÃ³rio** normalizar os dados (ex: `MinMaxScaler` entre 0 e 1). LSTMs nÃ£o convergem bem com dados em escala monetÃ¡ria bruta (ex: R$ 30,00).
* **Janelamento (Sliding Window):**
    * O problema deve ser modelado como aprendizado supervisionado.
    * **Feature (X):** SequÃªncia dos Ãºltimos *N* dias (ex: 60 dias).
    * **Target (y):** PreÃ§o do dia seguinte (T+1).
* **DivisÃ£o de Dados:**
    * **NÃƒO** utilizar `train_test_split` com `shuffle=True`.
    * A divisÃ£o deve ser **cronolÃ³gica** (ex: Treino: 2018-2023, Teste: 2024 em diante) para evitar *data leakage* (vazamento de dados futuros).

---

## 4. ğŸ§  Modelagem: Deep Learning

### 4.1. Arquitetura da Rede
* **Tipo:** Recorrente (RNN) com cÃ©lulas **LSTM**.
* **Input Shape:** `(Batch_Size, Timesteps, Features)`. Ex: `(32, 60, 1)`.
* **Camadas Ocultas:** Pelo menos uma camada LSTM.
* **RegularizaÃ§Ã£o:** Uso obrigatÃ³rio de **Dropout** (ex: 0.2) apÃ³s as camadas LSTM para prevenir overfitting.
* **SaÃ­da:** Camada Densa (`Dense`) com 1 neurÃ´nio e ativaÃ§Ã£o linear (para regressÃ£o).

### 4.2. CompilaÃ§Ã£o e Treino
* **FunÃ§Ã£o de Perda (Loss):** MSE (Mean Squared Error).
* **Otimizador:** Adam (Recomendado por adaptar o *learning rate*).
* **MÃ©tricas de Monitoramento:** MAE, Loss.

---

## 5. ğŸ“Š AvaliaÃ§Ã£o e MÃ©tricas

O modelo deve ser avaliado no conjunto de teste (dados nunca vistos) utilizando as seguintes mÃ©tricas obrigatÃ³rias:
1.  **RMSE (Root Mean Squared Error):** Penaliza grandes erros.
2.  **MAE (Mean Absolute Error):** Erro mÃ©dio absoluto na unidade monetÃ¡ria.
3.  **MAPE (Mean Absolute Percentage Error):** Erro percentual mÃ©dio (fÃ¡cil interpretaÃ§Ã£o para o negÃ³cio).

> **VisualizaÃ§Ã£o:** Deve ser gerado um grÃ¡fico de linha comparando a sÃ©rie temporal real vs. a sÃ©rie predita pelo modelo.

---

## 6. ğŸš€ Arquitetura de Software (Clean Code)

O projeto deve evitar o "Jupyter Notebook Driven Development" em produÃ§Ã£o. Sugere-se a seguinte estrutura:

```text
techChallenge-fase4/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md               # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ Dockerfile              # Receita da imagem da API
â”œâ”€â”€ docker-compose.yml      # (Opcional) OrquestraÃ§Ã£o API + MLflow
â”œâ”€â”€ requirements.txt        # DependÃªncias
â”œâ”€â”€ notebooks/              # Apenas para exploraÃ§Ã£o e grÃ¡ficos
â”‚   â””â”€â”€ exploratory.ipynb
â”œâ”€â”€ src/                    # CÃ³digo Fonte Modular
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # ConfiguraÃ§Ãµes (ticker, datas, caminhos)
â”‚   â”œâ”€â”€ data.py             # Download e PrÃ©-processamento
â”‚   â”œâ”€â”€ model.py            # DefiniÃ§Ã£o da classe/funÃ§Ã£o do modelo LSTM
â”‚   â”œâ”€â”€ train.py            # Pipeline de treinamento (com MLflow)
â”‚   â””â”€â”€ predict.py          # LÃ³gica de inferÃªncia (carrega modelo + scaler)
â””â”€â”€ api/                    # AplicaÃ§Ã£o Web
    â”œâ”€â”€ app.py              # Entrypoint (FastAPI)
    â””â”€â”€ schemas.py          # ValidaÃ§Ã£o de dados de entrada/saÃ­da
```

---

## 7. ğŸš¢ Deploy e EntregÃ¡veis

### 7.1. API (Backend)
Desenvolver uma API REST com os seguintes requisitos:

Endpoint /predict: Recebe os dados (ou busca internamente) e retorna o preÃ§o previsto.

A API deve carregar o modelo treinado e o Scaler salvo anteriormente para desnormalizar a previsÃ£o.

### 7.2. Docker
Criar um Dockerfile que instale as dependÃªncias e exponha a porta da API.

A aplicaÃ§Ã£o deve rodar com um simples comando docker run.

### 7.3. Lista de EntregÃ¡veis
Link do RepositÃ³rio Git: CÃ³digo organizado e limpo.

VÃ­deo Demo: Explicando a arquitetura, o modelo e mostrando a API funcionando.

Link da API em ProduÃ§Ã£o: Deploy em nuvem (Render, AWS, Azure, etc) OU instruÃ§Ãµes claras para rodar localmente via Docker.

---

## 8. ğŸ“š Conceitos das Aulas Aplicados
Redes Recorrentes (RNNs): Entendimento de memÃ³ria sequencial.

LSTM: SoluÃ§Ã£o para o problema do gradiente que desaparece (Vanishing Gradient) em sÃ©ries longas.

NormalizaÃ§Ã£o: Impacto direto na convergÃªncia do Gradient Descent.

RegularizaÃ§Ã£o (Dropout): TÃ©cnica para melhorar a generalizaÃ§Ã£o do modelo.

Otimizadores (Adam): EficiÃªncia no ajuste de pesos da rede neural.